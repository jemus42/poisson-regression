---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Anwendung

Als Beispiel verwenden wir hier `rwm1984`, ein Subset des Datensatz `rwm5yr` (vgl. Abschnitt \@ref(data), `?COUNT::rwm1984`), der Angaben zur Anzahl der Arztbesuche pro Person mit zusätzlichen demographischen Merkmalen enthält. Für unser Beispielmodell verwenden wir folgende Variablen:

- `docvis`: Abhängige Variable, Anzahl der Arztbesuche im Jahr (0-121)
- `outwork`: Arbeitslos (1), arbeitend (0)
- `age`: Alter (25 - 64)

```{r poisson-example-rwm}
# Daten (bemerke: rwm1984 ist kein exportiertes Objekt in COUNT)
data(rwm1984, package = "COUNT")

# Model fit
mod_rwm <- glm(docvis ~ outwork + age, family = poisson(), data = rwm1984)

# Summary display
pander(mod_rwm)
pander(glance(mod_rwm))
```

Das erste was wir zur Evaluation unseres Modells tun können, ohne direkt andere Modelle zum vergleich heranzuziehen, ist die beobachteten Counts und die auf Basis des Modells erwarteten Counts zu vergleichen, um ein grobes Gefühl für die Situation zu erhalten (Code frei adaptiert nach @hilbeModelingCountData2014, p. 88f):

```{r diag_compare_expected}
# Beobachtete und erwartete counts
observed_v_expected <- rwm1984 %>%
  count(docvis, name = "observed") %>%
  mutate(
    expected = purrr::map_dbl(docvis, ~{
                  dpois(.x, fitted(mod)) %>%
                    sum() %>%
                    round(2)
                }),
    difference = observed - expected
  ) 

observed_v_expected %>%
  head(5) %>%
  pander(caption = "Observed and expected counts")

# Mittelwert und Varianz der jeweiligen counts
# (für "expected" gilt Varianz := Mittelwert)
tribble(
  ~Counts,   ~Mean,                 ~Variance,
  "observed", mean(rwm1984$docvis), var(rwm1984$docvis),
  "expected", mean(fitted(mod)),    mean(fitted(mod))
) %>%
  pander(caption = "Mean & variance of observed and expected counts")

# Plot: observed vs. expected counts
observed_v_expected %>%
  filter(docvis <= 12) %>%
  gather(type, count, observed, expected) %>%
  ggplot(aes(x = docvis, y = count, fill = type, color = type)) +
  geom_point(shape = 21, color = "black", stroke = .5, size = 2) +
  geom_path(linetype = "dotted", size = .25) +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_fill_brewer(palette = "Set1", aesthetics = c("color", "fill"), name = "") +
  labs(
    title = "rwm1984: Poisson Modell",
    subtitle = "Observed und expected counts bei overdispersion",
    caption = "Output begrenzt auf docvs <= 12",
    x = "Anzahl der Arztbesuche (docvis)", y = "Count"
  ) +
  theme(legend.position = "top")
```

Anhand der ersten Tabelle können wir recht schnell erkennen, dass wir hier *deutlich* mehr Nullen beobachten als das Modell vorhersagt – mehr dazu in Abschnitt \@ref(zeros).  
Der Plot veranschaulicht den eher suboptimalen model fit unter diesen Umständen (overdispersion & zero-inflation).


## Overdispersion: Erkennung und Handhabung {#overdisp-diag}

Der *Score-Test* wird nicht unbedingt empfohlen, findet sich aber in der Literatur:

```{definition, name = "Score-Test auf Overdispersion"}
Nach @hilbeModelingCountData2014 (p. 85f):
  
Sei $Y$ die abhängige Variable und $y_i$ die $i-te$ Beobachtung, mit $\hat{\mu}_i$ als gefitteter Wert eines Poisson-Modells, dann wird der z-Score wie folgt berechnet:

\begin{equation}
  z_i = \frac{(y_i - \hat{\mu})^2 -y}{\hat{\mu} \sqrt{2}}
\end{equation}
  
Zur Evaluation von $z$ wird z.B. ein Intercept-only Regressionsmodell mit $z$ als abhängiger Variable gefitted, mit der Nullhypothese "Es liegt keine Overdispersion vor".

Dabei gelten folgende Annahmen:
  
1. Die Stichprobe ist *groß*
2. $z$ ist t-verteilt

```



```{definition, name = "Lagrange Multiplier Test"}
Nach @hilbeModelingCountData2014 (p. 85f):

Eine $\chi^2$-Teststatistik mit einem Freiheitsgrad.
  
\begin{equation}
  \chi^2 = \frac{\left( \sum_{i=1}^n \hat{\mu}_i^2 - n \bar{y} \right)^2}
                {2 \sum_{i=1}^n \hat{\mu}_i^2}
\end{equation}

```

Eine rudimentäre R-Implementation findet sich in Abschnitt \@ref(helperfuns).  


### (Boundary) Likelihood Ratio Test (BLR)

Zum vergleich von zwei geschachtelten (*nested*) Modellen kann der *Likelihood Ratio Test* verwendet werden:

```{definition LRT, name = "Likelihood Ratio Test"}
\begin{equation}
 LR = -2 (\mathcal{L}_R - \mathcal{L}_F)
\end{equation}
 
Mit $\mathcal{L}_F$ als log-likelihood des vollen (oder "größeren") Modells, und  $\mathcal{L}_R$ als log-likelihood des reduzierteren Modells. 
```

Eine Variante des Tests kann verwendet werden, um den Dispersionsparameter $\alpha$ eines NB-Modells zu testen. Da eine NB-Verteilung für $\alpha = 0$ äquivalent zur Poisson ist (siehe Abschnitt \@ref(mod-nb)), kann ein Poisson-Modell als reduzierte Variante eines NB-Modells betrachtet werden. In diesem Fall verwendet man den *Boundary Likelhihood Ratio* test:

```{definition BLR, name = "Boundary Likelihood Ratio Test"}
\begin{equation}
 BLR = -2 (\mathcal{L}_\mathrm{Poisson} - \mathcal{L}_\mathrm{NB})
\end{equation}
```

> It is important, though, to remember that the BLR test has a lower limiting case for the value of $\alpha$, which is what is being tested. Given that the standard parameterization of the negative binomial variance function is $\mu + \alpha \mu^2$ , when $\alpha = 0$, the variance reduces to $\mu$.
>
> --- @hilbeModelingCountData2014, p. 115

Der resultierende Wert ist $\chi^2_{(1)}$-verteilt. Der resultierende p-Wert muss zusätzlich halbiert werden [siehe @hilbeModelingCountData2014, p. 115].

> Since the distribution being tested can go no lower than 0, that is the boundary. Only one half of the full distribution is used. Therefore the Chi2 (sic!) test is divided by 2
>
> --- @hilbeModelingCountData2014, p. 115


Am Beispiel der `rwm1984`-Daten:

```{r blr}
# Poisson-Modell
mod_p <- glm(docvis ~ outwork + age, family = poisson(), data = rwm1984)

# NB-Modell
mod_nb <- MASS::glm.nb(docvis ~ outwork + age, data = rwm1984)

# BLR: Recht eindeutig.
lmtest::lrtest(mod_p, mod_nb)

# Bei gegebenem BLR-Wert von 4.2 ließe sich der p-Wert wie folgt berechnen:
pchisq(4.2, df = 1, lower.tail = FALSE) / 2
# Oder
(1 - pchisq(4.2, df = 1)) / 2
```

### Umgang mit Overdispersion

Zur expliziten Modellierung des Dispersionparameters kann ein NBH Modell (\@ref(mod-nbh)) genutzt werden, falls die Quelle der overdispersion von besonderem Interesse ist.  

Abseits davon bleiben 2 grobe Strategien : 

1. Adjustierung der durch die overdispersion verzerrten Standardfehler (Quasipoisson, Quasi-Likelihood, robuste Varianzschätzer)
2. Wechsel auf ein Modell, das overdispersion (oder allgemeine extradispersion) erlaubt (e.g. NB)

(Bemerke: Standardfehler für IRRs werden i.A. über die *delta method* bestimmt, die ich noch recherchieren und irgendwo einbauen muss)

##### Quasipoisson: Skalierung der Standardfehler {-}

Hier werden lediglich die Standardfehler der Koeffizienten eines Poisson-Modells adjustiert, die bei overdispersion i.d.R. unterschätzt werden – die eigentliche Parameterschätzung wird nicht beeinflusst:

$$\mathrm{SE}(\beta_k) \cdot \sqrt{D}$$
Wobei $D$ der Pearson-Dispersionsindex ist (vgl. @hilbeModelingCountData2014, p. 92ff), den wir in \@ref(def:def-pearsondisp) als $\frac{\chi^2_{\mathrm{Pearson}}}{\mathrm{df}}$ definiert hatten.

Ein möglicher Nachteil jedoch: Es wird zuerst ein reguläres Poisson-Modell gefittet, Standardfehler und Dispersionsindex bestimmt, und dann dasselbe Modell mit skalierten Standardfehlern erneut gefittet – dementsprechend ist diese Methode vermutlich für größere Datensätze eher ineffizient.

Einmal auf unser voriges Beispiel anhand der `rwm5yr`-Daten angewandt:

```{r quasipois-example}
# Urprüngliches Model
mod <- glm(docvis ~ outwork + age, family = poisson(), data = rwm1984)

mod_qp <- glm(docvis ~ outwork + age, family = quasipoisson(), data = rwm1984)

dispersion(mod)
dispersion(mod_qp)
```

An der Dispersion hat sich nichts geändert, nur an den Standardfehlern der Koeffizienten (in folgenden Tabellen auf log-Skala):

```{r quasipois-example2}
pander(mod, caption = "Poisson-Modell")
pander(mod_qp, caption = "Quasipoisson-Modell")
```

In diesem Fall "lohnt" sich dieser Ansatz zwar eher nicht, da wir neben overdispersion noch ein Problem mit zero-inflation haben, aber in manchen Fällen kann Quasipoisson-Methode ausreichen.

> "Scaling is an easy, quick-and-dirty method of adjusting standard errors for overdispersion. However, when data are highly correlated or clustered, model slopes or coefficients usually need to be adjusted as well. Scaling does not accommodate that need, but simulations demonstrate that it is quite useful for models with little to moderate overdispersion."
>
> --- @hilbeModelingCountData2014 (p. 96)

#### Robuste Varianzschätzer (Sandwich Estimators) {#sandwich}

(Andere Namen: Huber & White standard errors, empirical standard errors)

Sandwich estimators sind in erster Linie für nicht unabhängige bzw. korrellierte Daten gedacht, zum Beispiel für Daten, die innerhalb mehrere Haushalte, Krankenhäuser, Städte etc. gesammelt wurden.

Robuste Varianzschätzer können (und je nach Quelle: *sollten*) standardmäßig für Count-response Daten verwendet werden, da die resultierenden Schätzer im Falle tatsächlich unkorrellierter Daten äquivalent zu den Standardfehlern des ursprünglichen Modells sind (i.e. "schadet nicht").

Bootstrapped SEs wiederum erfordern mehr Aufwand, ähneln aber den robusten SEs sowieso sehr stark, weshalb sich der Aufwand ggf. nicht wirklich lohnt.

Siehe @hilbeModelingCountData2014 (p. 99f) für eine ausführlichere Beschreibung. Zudem gibt der Autor explizit den Rat:

> "Unless your Poisson or negative binomial model is well fitted and meets its respective distributional assumptions, use robust or empirical standard errors as a default"
>
> --- @hilbeModelingCountData2014 (p. 133)


## Zero-Inflation (ZI) {#zeros}

Problem: "Excess zeros", i.e. das Modell sagt für $P(y_i = 0\ |\ x_i)$ eine deutlich kleinere Wahrscheinlichkeit vor, als in gegebenen Daten tatsächlich vorliegen.

Zero-inflation hängt eng mit overdispersion zusammen. Sie *kann* Ursache für overdispersion sein, allerdings bedeutet das nicht direkt, dass auch ein entsprechend auf *ZI*-fokussiertes Modell (ZIP, ZINB...) verwendet werden *muss* – ggf. lässt sich die overdispersion bereits durch ein anderes geeignetes Modell "auffangen". Zum Beispiel zeigt @winkelmannEconometricAnalysisCount2010, dass die *NB* allgemein eine größere Anzahl an Nullen erwartet als die Poisson.


```{r excess_zeros_viz, echo=FALSE}
tibble(
  x = 0:15,
  pois = dpois(x, 5) * 100,
  y0 = round(ifelse(x == 0, 5, pois)),
  ymod = dpois(x, mean(y0)) * 100
) %>%
  gather(dist, prob, y0, ymod) %>%
  ggplot(aes(x = x, y = prob, fill = dist)) +
  geom_col(position = "dodge", alpha = .75) +
  scale_x_continuous() +
  scale_fill_brewer(palette = "Set1", labels = c("y0" = "Beobachtet", "ymod" = "Fitted")) +
  labs(
    title = "Zero-Inflation: Simuliertes Beispiel",
    subtitle = "Beobachtete und gefittete Werte bei excess zeros",
    x = "Y", y = "Count", fill = ""
  ) +
  annotate("text", label = "Nullen beobachtet", x = -.3, y = 9, 
           angle = 270, hjust = 1) +
  annotate("text", label = "Nullen erwartet", x = .3, y = 9, 
           angle = 270, hjust = 1) +
  annotate("segment", x = -0.25, xend = -0.25, y = 8.5, yend = 5.5, 
           arrow = arrow(angle = 40, length = unit(.2, "cm")),
           lineend = "round", linejoin = "mitre") +
  annotate("segment", x = 0.25, xend = 0.25, y = 8.5, yend = 1, 
           arrow = arrow(angle = 40, length = unit(.2, "cm")),
           lineend = "round", linejoin = "mitre") +
  theme(legend.position = "top")
```


### Tests auf Zero-Inflation {-}

Zwar schlagen @perumean-chaneyZeroinflatedOverdispersedWhat2013 den Vuong Test (siehe @vuongLikelihoodRatioTests1989 und insbesondere @desmarais2013TestingZero zur Anwendung) vor, um reguläre Modelle mit ihren zero-inflated Gegenstücken zu vergleichen (e.g Poisson vs. ZIP oder NB vs. ZINB), allerdings spricht sich @wilson2015MisuseVuong explizit *gegen* den Vuong-Test aus, da die Vorraussetzungen für den Test (ungeschachtelte Modelle) verletzt seien:

> The misuse of the test stems from misunderstanding of what is meant by the terms "non-nested model" and "nested model". As is the case with many frequently used terms their meanings are approximately understood by many, but precisely understood by few.
>
> --- @wilson2015MisuseVuong, p. 52


(Hausman Test: Vergleich der Koeffizienten zweier Modelle, taucht in Hilbe (2014) aber eher im Kontext von Validation samples auf)

### Modellierung {-}

Für die Modellierung von zero-inflated Daten stehen primär zwei Möglichkeiten zur Verfügung (vgl. auch @hilbeModelingCountData2014, p. 19):

1. *Hurdle models*: Getrennte Modellierung von Nullen und positiven counts in zwei separaten Modellen (siehe Abschnitt \@ref(mod-hurdle)).
2. *Zero-inflated models*: Mixture models, die aus zwei Wahrscheinlichkeitsfunktionen eine neue Wahrscheinlichkeitsfunktion generieren, die sowohl Nullen als auch positive counts beschreibt, allerdings nun auch einen *zero-inflation*-Parameter hat (siehe Abschnitt \@ref(mod-zi)).

Ob hurdle model oder zero-inflated model die bessere Wahl ist hängt mitunter davon ab, welche Annahmen über die _Ursache der Nullen_ getroffen werden können.  
Wenn es eine echte Trennung der Mechanismen (*"separation of mechanisms"*) gibt, die die Nullen und die positiven counts verursachen, dann wäre ein hurdle model eher angemessen.  
Wenn sich die Nullen überlappen, es also keine getrennten Prozesse zu geben scheint, dann wären zero-inflated models angemessen [@hilbeModelingCountData2014, p. 209].

Als Veranschaulichung dieser getrennten Mechanismen können wir das *fish*-Beispiel auf [dieser UCLA-Tutorialseite verwenden](https://stats.idre.ucla.edu/sas/output/zero-inflated-poisson-regression/). Hier ist die Anzahl der gefangenen Fische an einem Wochenende in einem Park die Zählvariable.

```{r fishing_zeros}
# Mittelwert & Varianz von 'count'
describe_counts(fish$count)

# Barchart für counts <= 50, Spannweite sehr groß
fish %>%
  filter(count <= 50) %>%
  ggplot(aes(x = count)) +
  geom_bar(alpha = .75) +
  scale_x_continuous() +
  labs(
    title = "'fish': Anzahl gefangener Fische",
    subtitle = "Limitiert auf counts unter 50",
    x = "Anzahl gefangener Fischer", y = "Count"
  )
```

Die Anzahl der Nullen scheint hier auffallend groß zu sein, allerdings beobachten wir hier auch zwei unterschiedliche Mechanismen: Eine Gruppe kann das ganze Wochenende geangelt haben, während eine andere Gruppe gar nicht geangelt hat – beide Gruppen werden jedoch am Ende nach der Anzahl ihrer gefangenen Fische befragt.  

