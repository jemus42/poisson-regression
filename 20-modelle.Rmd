---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Mehrparametrische Modelle {#multiparam}

```{r model-graph, echo=FALSE, cache=FALSE, fig.cap="Hierarchie ausgewählter Count-Modelle. In Klammern: Zu schätzende Parameter der zugrundeliegenden Verteilung"}
grViz(readLines("graphs/model-graph.dot"))
```


Zweiparametrische Modelle haben neben dem Parameter für den Erwartungswert einen weiteren Parameter für die Dispersion, was wir natürlich insbesondere im Kontext der Dispersionsproblematik sehr nützlich finden.  
Alle hier aufgeführten Modelle (inklusive der zero-inflation models) können als Verallgemeinerung der Poisson-Verteilung um (mindestens) einen weiteren Parameter aufgefasst werden. Die Unterschiede liegen hauptsächlich in der Parametrisierung und den damit zusammenhängenden Einschränkungen. Die *Negative Binomialverteilung* und die *Poisson Inverse Gaussian* zum Beispiel erweitern beide die Poisson um einen Dispersionsparameter, machen aber unterschiedliche Annahmen zur Verteilung der Varianz (gamma- vs. invers-normalverteilt).  

Tabelle \@ref(tab:modelstable) zeigt eine Reihe von Verteilungen mit entsprechend parametrisierter Varianz abhängig vom Erwartungswert $\mu$ und einem Dispersionsparameter $\alpha$.

```{r modelstable, echo=FALSE}
tibble::tribble(
  ~Model,                         ~Mean,  ~Variance,
  "Poisson",                     "$\\mu$", "$\\mu$",
  "Negative Binomial I (NB1)",   "$\\mu$", "$\\mu (1 + \\alpha) = \\mu + \\alpha\\mu$",
  "Negative Binomial II (NB2)",  "$\\mu$", "$\\mu (1 + \\alpha\\mu) = \\mu + \\alpha\\mu^2$",
  "Negative Binomial-P (NBP)",   "$\\mu$", "$\\mu (1 + \\alpha\\mu^p) = \\mu + \\alpha\\mu^p$",
  "Poisson Inverse Gaussian (PIG)", "$\\mu$", "$\\mu (1 + \\alpha\\mu^2) = \\mu + \\alpha\\mu^3$",
  "Generalized Poisson (GP)",    "$\\mu$", "$\\mu (1 + \\alpha\\mu)^2 = \\mu + 2\\alpha\\mu^3 + \\alpha^2\\mu^3$"
 ) %>%
  kable(booktabs = TRUE, escape = FALSE, linesep = "",
        caption = " Ein Überblick einiger Modelle mit Varianzparametrisierung (nach @hilbeModelingCountData2014, p. 12).") %>%
  kable_styling(position = "center", protect_latex = FALSE)
```


## Negativ Binomial (NB) {#mod-nb}

Die Negative Binomialverteilung (*NB*) kann als zweiparametrische Erweiterung der *Poisson* betrachtet werden, wobei die Varianz (separat vom Erwartungswert) als Gamma-verteilte Zufallsvariable betrachtet wird (*Poisson-Gamma Mischmodell*).  
Für eine ausführliche Beschreibung, siehe @hilbeModelingCountData2014 (p. 126ff).

Die Wahrscheinlichkeitsfunktion mit der $\alpha$-Parametriserung:

```{definition defnb2alpha, name = "Negative Binimialverteilung 2, alpha"}
Nach [@hilbeModelingCountData2014 p. 130]:

Für $Y \sim NB(\mu, \alpha)$ gilt

\begin{equation*}
  P(Y = y_i) = \begin{pmatrix} y_i + \frac{1}{\alpha} - 1 \\ \frac{1}{\alpha} - 1\end{pmatrix} \left(\frac{1}{a + \alpha \mu_i}\right)^{\frac{1}{\alpha}} \left(\frac{\alpha \mu_i}{1+\alpha\mu_i}\right)
\end{equation*}

Mit $\mathbb{E}(Y) = \mu$ und $\mathrm{Var}(Y) = \mu + \alpha \mu^2$

```

Eine Alternative unter Verwendung von $\theta = \frac{1}{\alpha}$:

```{definition defnb2theta, name = "Negative Binomialverteilung 2, theta"}
Nach @perumean-chaneyZeroinflatedOverdispersedWhat2013 (p. 1675):

Für $Y \sim NB(\mu, \theta)$ gilt
  
\begin{equation*}
P(Y = y) = \frac{\Gamma(\theta + y)}{\Gamma(\theta) \Gamma(y+1)} \frac{\theta^\theta \mu^y}{(\theta + \mu)^{(\theta + y)}}, \quad y = 0, 1, 2, \ldots
\end{equation*}

Mit $\mathbb{E}(Y) = \mu$ und $\mathrm{Var}(Y) = \mu + \frac{\mu^2}{\theta}$
```

Der gängigste Anwendungsfall der *NB* findet sich bei Daten mit nicht korrigierbarer overdispersion (siehe Abschnitt \@ref(overdispersion)), da der Parameter $\theta$ bzw. auch $\alpha = \frac 1 \theta$ als *Dispersionsparameter* dient:

```{definition nbdisppar, name = "NB-Dispersionsparameter"}

Meist wird der Parameter als $\alpha$ bezeichnet, mit der Interpretation (relativ zum Poisson-Modell):
  
$$\mathrm{Var}(Y) = \mu + \alpha \mu^2$$

\begin{align*}
  \alpha &= 0 &&\Longrightarrow \text{ Äquivalent zur Poissonverteilung} \\
  \alpha &> 0 &&\Longrightarrow \text{ Overdispersion}
\end{align*}

Je nach Quelle (und unter Anderem in R (z.B. `MASS::glm.nb`)) wird die inverse Variante $\theta = \frac 1 \alpha$ verwendet, womit gilt:

$$\mathrm{Var}(Y) = \mu + \frac{\mu^2}{\theta}$$

\begin{align*}
  \theta &> 0 &&\Longrightarrow \text{ Overdispersion} \\
  \theta &\to \infty &&\Longrightarrow \text{ Äquivalent zur Poissonverteilung}
\end{align*}

Siehe dazu auch @hilbeModelingCountData2014 (p. 131).

```

Das GLM sieht keinen weiteren zu schätzenden Parameter vor, weshalb $\theta$ bzw. $\alpha$ in der Praxis separat geschätzt wird ^[Das ist auch der Grund, warum in R nicht `glm()` verwendet werden kann, sondern eine separate Implementation mit expliziter Schätzung von $\theta$ benötigt wird (`MASS::glm.nb`)].

Der wohl wichtigste Aspekt des Dispersionsparameters, unabhängig davon ob $\alpha$ oder $\theta$ verwendet wird, ist sein Vorzeichen: Er ist _immer_ positiv, das heißt die Varianz der NB ist entweder _größer oder gleich_ der Poisson, oder mit anderen Worten:

> "The negative binomial model adjusts for Poisson overdispersion; it **cannot be used to model underdispersed** Poisson data" 
>
> --- [@hilbeModelingCountData2014 (p. 11), eigene Hervorhebung]

Je nach Software/Algorithmus ist es auch möglich, dass ein NB-fit nicht möglich ist, wenn $\alpha \approx 0$ (die Verteilung zu nah an Poisson) bzw. die Daten Poisson-underdispersed sind. 

Weiterhin gibt es zwei verschiedene Formulierungen der Negativen Binomialverteilung, NB1 und NB2, deren Nummerierung auf den Exponenten im zweiten Term ihrer Varianzen zurückzuführen ist:

```{definition defNB1NB2, name = "NB1 und NB2"}
Nach @hilbeModelingCountData2014 (p. 126f):

Die **NB1**, oder auch *lineare* Negative Binomialverteilung hat die Varianz

\begin{equation*}
\mathrm{Var}(Y) = \mu + \alpha \mu
\end{equation*}

…und die gängigere **NB2**, oder auch *quadratische* Negative Binomialverteilung hat (wie oben beschrieben) die Varianz

\begin{equation*}
\mathrm{Var}(Y) = \mu + \alpha \mu^2
\end{equation*}

```

Beide Varianten können mittels MLE geschätzt werden, wobei NB2 zusätzlich im Kontext des GLM via [IRLS](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares) geschätzt werden kann.

Als link wird $\log \mu$ verwendet, wobei der *canonical link* eigentlich $\log\left(\frac{\alpha\mu}{1 + \alpha\mu}\right)$ ist [^nblink]. 

[^nblink]: Daher können Standardfehler nicht wie für canonical models üblich via OIM (*observed information matrix*) geschätzt werden, sondern via EIM (*expected information matrix*). Die EIM-Methode liefert weniger genaue Resultate für kleine Stichproben (n < 30). So jedenfalls @hilbeModelingCountData2014.

Für ein gut angepasstes NB-Modell gilt analog eines Poisson-Modells, dass die Dispersionsstatistik approximativ 1 ist (siehe \@ref(def:def-pearsondisp)).

Zur Anwendung in R gibt es zwei Möglichkeiten: `MASS::glm.nb` oder `msme::nbinomial`, wobei letztere mehrere Optionen zur Parametrisierung hat und zusätzlich die Dispersionsstatistik ausgibt:

```{r nb_r, warning=FALSE}
mod_nb <- MASS::glm.nb(docvis ~ outwork + age,
                       data = rwm1984)
summary(mod_nb)


mod_nb2 <- msme::nbinomial(docvis ~ outwork + age,
                           data = rwm1984)
summary(mod_nb2)
```



## Poisson Inverse Gaussian (PIG) {#mod-pig}

So wie die *NB* als mixture aus Poisson-Verteilung mit Gamma-verteilter Varianz betrachtet werden kann, ist die *Poisson Inverse Gaussian* (*PIG*) eine mixture aus Poisson-Verteilung mit [invers-normalverteilter](https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution) Varianz.  
Die Verteilung hat Erwartungswert und Varianz mit gleicher Form wie die NB2, unterscheidet sich aber in ihrer Form – sie ist linkssteiler und hat längere tails.

Die *PIG* kann also alternativ zur *NB* verwendet werden, insbesondere für den Fall sehr linkssteiler Daten.

> "Simply put, PIG models can better deal with highly overdispersed data that can negative binomial regression, particularly data clumped heavily at 1 and 2"
> 
> -- @hilbeModelingCountData2014 (p. 163)

Siehe auch 

**Software**:

- R: `gamlss` (CRAN)
- SAS: Machbar via `NLMIXED`, aber unschön [siehe @high2018AlternativeVariance].
- Stata: `pigreg`

Vergleichen wir ein PIG Modell mit unserem vorigen NB2-Modell:

```{r pig-example}
# Poisson Modell
mod_pois <- glm(docvis ~ outwork + age, family = poisson(),
              data = rwm1984)
# NB2 Modell
mod_nb <- MASS::glm.nb(docvis ~ outwork + age,
                       data = rwm1984)

# PIG Modell
mod_pig <- gamlss::gamlss(docvis ~ outwork + age, family = gamlss.dist::PIG,
                          data = rwm1984)
summary(mod_pig)

cbind(
  exp(coef(mod_pois)),
  exp(coef(mod_nb)),
  exp(coef(mod_pig))
) %>%
  as_tibble(rownames = ".id") %>%
  setNames(c("Koeffizient", "Poisson", "NB2", "PIG"))
```

Zum Modellvergleich kann das AIC verwendet werden:

```{r pig-example-aic}
AIC(mod_pois, mod_nb, mod_pig)
```


## Generalized Poisson (GP) {#mod-gp}

Die Verteilung besitzt auch einen zusätzlichen Dispersionsparameter (analog NB, PIG), allerdings kann dieser hier auch negative Werte annehmen, womit die GP für alle Dispersionsarten (bzw. insbesondere underdispersion) geeignet ist.

```{definition def-genois, name = "Generalized Poisson"}
Aus @hilbeModelingCountData2014 (p. 211) und @harris2012ModelingUnderdispersed:

Eine Variable $Y$ ist *generalized Poisson* verteilt mit der PMF:
  
\begin{equation*}
P(Y = y_i) = \frac{\theta_i \left(\theta_i + \delta y_i\right)^{y_i - 1}}{y_i !} \exp(-\theta_i - \delta y_i), \quad y_i = 1, 2, 3, \ldots
\end{equation*}

\begin{align*}
\theta_i > 0, \quad \max(-1, \frac{-\theta_i}{4}) < \delta < 1 \\[2em]
\mu_i = \mathbb{E}(Y_i) &= \frac{\theta_i}{1 - \delta} \\
\mathrm{Var}(Y_i) &= \frac{\theta}{(1 - \delta)^3} 
                   = \frac{1}{(1 - \delta)^2} \mathbb{E}(Y_i)
                   = \Phi \mathbb{E}(Y_i)
\end{align*}

Wobei $\Phi = \frac{1}{(1 - \theta)^2}$ als Dispersionsparameter dient.

Es gilt außerdem

\begin{align*}
\delta = 0 &\Longrightarrow \text{Equidispersion (Poisson)} \\
\delta < 0 &\Longrightarrow \text{Underdispersion} \\
\delta > 0 &\Longrightarrow \text{Overdispersion}
\end{align*}

Auch hier kann ein Likelihood-Ratio Test auf $\delta = 0$ durchgeführt werden, analog $\alpha = 0$ für die NB – wobei $\delta$ hier keine Restriktion hat (siehe BLR, Definition \@ref(def:BLR)).

```

Eine alternative Darstellung unter Verwendung von Parametern $\mu$ und $sigma$ findet sich in @stasinopoulos2007GeneralizedAdditive, der Grundlage für das R-package `gamlss` (*"Generalized additive models for location, scale and shape"*):

```{definition defgpois2, name = "Generalized Poisson (GAMLSS)"}
\begin{equation*}
f(y\ |\ \mu,\sigma)= \left(\frac{\mu}{(1+\sigma+\mu)}\right)^y \frac{(1+\sigma y)^{(y-1)}}{y!} \exp\left(-\mu \frac{(1+\sigma y)}{(1+\sigma+\mu)}\right)
\end{equation*}

Für $y=0,1,2,\ldots$ mit $\mu > 0$ und $\sigma > 0$.
```

**Software**:  

- R: `VGAM`: `vgam(..., family = genpoisson())`]
    - Hier wird das obige $\delta$ als $\lambda$ bezeichnet (Verwechslungsgefahr mit Poisson-Parameter)
    - Numerisch instabil für $\lambda$ nahe 0 oder 1
    - Siehe [`?VGAM::genpoisson`](https://rdrr.io/cran/VGAM/man/genpoisson.html)
- Alternativ via [gamlss](https://www.gamlss.com/): `gamlss(..., family = GPO)`

In beiden R-Varianten habe ich bisher nicht geschafft $\theta$, den Dispersionsparameter, zu extrahieren – auch wenn die Koeffizientien aus dem Stata-Beispiel in [@hilbeModelingCountData2014, p. 213f] reproduzierbar sind.

- SAS: `NLMIXED` oder `FMM`, siehe auch [SAS support document](http://support.sas.com/kb/56/549.html)

Allgemein scheint die Parametrisierung (und Notation der Parameter) zu variieren.


## Conway-Maxwell Poisson (COMP) {#mod-comp}

```{definition, name = "Conway-Maxwell-Poisson Modell (CMP)"}
Nach @shmueli2005UsefulDistribution (p. 129):
  
\begin{align}
P(X = x) &= \frac{\lambda^x}{(x!)^\nu} \frac{1}{Z(\lambda, \nu)}, \quad x = 0, 1, 2, \ldots \\
Z(\lambda, \nu) &= \sum_{j = 0}^\infty \frac{\lambda^j}{(j!)^\nu} \\
\\
\lambda > 0&, \nu \ge 0
\end{align}

Mit (im Unterschied zur Poisson) nichtlinearer Zerfallsrate $\nu$ *rate of decay*, so dass:

\begin{equation}  
\frac{P(X = x -1)}{P(X = x)} = \frac{x^\nu}{\lambda}
\end{equation}

Der Zusammenhang zwischen $\nu$ und $\lambda$ nach @sellers2010FlexibleRegression (p. 946) legt Nahe, dass $\nu$ ähnlich $\alpha$ in NB-Modellen die Dispersion bestimmt:
  
\begin{align*}
  \mathrm{Var}(Y_i) =& \lambda_i \frac{\partial}{\partial \lambda_i} \mathbb{E}(Y_i)
            \approx \lambda_i \frac{\partial}{\partial \lambda_i} \left( \lambda_i^{\frac{1}{\nu}} - \frac{\nu - 1}{2 \nu} \right) \\
                  =& \frac{1}{\nu} \lambda^{\frac{1}{\nu}}
           \approx \frac{1}{\nu} \mathbb{E}(Y_i)
\end{align*}

Als link wird $\log \lambda$ verwendet, da so die links für Poisson- und logistische Regression Spezialfälle der COMP sind.
```

Die Verteilung hat als Spezialfälle:

- $\nu = 1 \Longrightarrow Z(\lambda, \nu) = exp(\lambda)$: Poisson
- $\nu \to \infty \Longrightarrow Z(\lambda, \nu) \to 1 + \lambda$: Bernoulli mit $P(X = 1) = \frac{\lambda}{1+\lambda}$
- $\nu = 0,\ \lambda < 1$: Geometrisch mit $P(X = x) = \lambda^x (1 - \lambda)$


**Vorteile**:

- "Brücke" zwischen logistischer und Poisson-Regression [@sellers2010FlexibleRegression]

> Although the logistic regression is a limiting case $(\nu \to \infty)$, in practice, fitting a COM-Poisson regression to binary data yields estimates and predictions that are practically identical to those from a logistic regression.
>
> --- @sellers2010FlexibleRegression, p. 945

- "Low cost", i.e. "nur" ein zusätzlicher Parameter
- Einfach zu handhaben (wenn in GLM mit MLE statt MCMC estimation)
- Over- und underdispersion abbildbar
- Zero-inflation abbildbar

**Software**:

- `CompGLM::glm.comp`: Funktioniert, aber crasht RStudio wenn `nuFormula` spezifiziert wird
- `compoisson::com.fit`: Entweder generell nicht geeignet oder unfassbar langsam (und deshalb nicht geeignet)
- `COMPoissonReg` (GitHub: `lotze/COMPoissonReg`): Scheint zu funktionieren

Da die COMP-Wahrscheinlichkeitsfunktion eine unendliche Summe $Z(\lambda, \nu)$ enthält, kann es bei der Anwendung in Software durchaus dazu kommen, dass keine Konvergenz erreicht wird.  Wie viele Iterationen für die Konvergenz der Summe benötigt werden hängt zum einen von $\nu$ ab: Je größer, desto schneller die Konvergenz. Das Gegenteil gilt für $\lambda$ – je größer, desto langsamer die Konvergenz [@high2018AlternativeVariance, p. 4]. 

Siehe auch

- @sellers2010FlexibleRegression für eine gute Übersicht
- @lord2010ExtensionApplication, @lord2008ApplicationConwayMaxwellPoisson für eine Anwendung
- @shmueli2005UsefulDistribution



## Diagnostische Modelle

### Heterogenous Negative Binomial (NBH) {#mod-nbh}

- Selling feature: Erlaubt Parametrisierung von $\alpha$
- -> Ursache für under-/overdispersion modellierbar

@hilbeModelingCountData2014 (p. 156)

```{r nbh, warning=FALSE}
data(nuts, package = "COUNT")
nuts <- subset(nuts, dbh < 0.6)

ggplot(data = nuts, aes(x = cones)) +
  geom_histogram(binwidth = 1)

# Poisson-Modell als Startpunkt
m_pois <- glm(cones ~ sntrees + sheight + scover, family = poisson(), data = nuts) 
dispersion(m_pois) # -> overdispersion 

# NB-Modell
m_nb <- msme::nbinomial(cones ~ sntrees + sheight + scover, data = nuts)
summary(m_nb) # Dispersion < 1 -> NB-underdispersed

# NB-H: Ursache der Dispersion?
m_nbh <- msme::nbinomial(cones ~ sntrees + sheight + scover, 
                         formula2 = cones ~ sntrees + sheight + scover,
                         scale.link = "log_s",
                         family = "negBinomial",
                         data = nuts)
summary(m_nbh)
```

Signifikanz der Dispersionsprädiktoren (Suffix `_s`) kann nun verwendet werden, um Ursachen der overdispersion zu identifizieren – auch wenn in diesem Beispiel keiner der Koeffizienten signifikant wird.

### Negative binomial-P (NB-P) {#mod-nbp}

Eine Erweiterung der NB1 und NB2, die den Exponenten im Varianzterm parametrisiert ($\rho$), womit die Dispersion nun nicht mehr statisch für alle Beobachtungen ist:

```{definition defnbp, name = "NB-P"}
\begin{align*}
  \text{NB-P: }\qquad \mathrm{Var}(Y) &= \mu + \alpha\mu^\rho \\
  \text{NB1: } \qquad \mathrm{Var}(Y) &= \mu + \alpha\mu      \\
  \text{NB2: } \qquad \mathrm{Var}(Y) &= \mu + \alpha\mu^2
\end{align*}
```

Damit ist NB-P äquivalent zu NB2 für $\rho = 2$ bzw. NB1 für $\rho = 1$.  
Um zu evaluieren, ob ein NB1 oder NB2 Modell einen besseren Fit liefert, kann man nun ein NB-P Modell zum Vergleich fitten und Likelihood Ratio Tests zum Vergleich heranziehen.

Siehe auch @hilbeModelingCountData2014 (p. 155) – wo der Autor zwar den obigen Ansatz beschreibt, aber zur Entscheidung zwischen NB1/2 zusätzlich auf die sowieso etablierten Informationskriteren (AIC/BIC) zurückgreift. Dazu gehört auch, dass das NB-P kein rein diagnostisches Werkzeug sein muss: Wenn ein NB-P ein _deutlich_ kleineres AIC aufweist als NB1/2-Modelle, kann das NB-P auch als Modell der Wahl in Erwägung gezogen werden.

## Zero-Inflated Models (*mixture models*) {#mod-zi}

Zero-inflated Modelle sind *mixtures*, das heißt, sie entstehen durch Mischung zweier Wahrscheinlichkeitsfunktionen. In der Regel wird eine Nullverteilung mit einer regulären Zählverteilung (Poisson, NB, PIG, ...) "gemischt", abhängig von einem *mixture parameter* $\pi \in [0,1]$, der z.B. logistisch modelliert wird. Die Nullen im Modell werden dementsprechend durch beide Verteilungen (Nullverteilung, Zählverteilung) beschrieben – im Gegensatz zu *hurdle models* (vgl. \@ref(mod-hurdle)), die Nullen logistisch und die positiven counts über eine *truncated* distribution modellieren würden.  

Zero-inflated distributions wie ZIP und ZINB wurden hergeleitet als zweiteilige mixture distributions. Die allgemeine Form für mixture distributions ist

```{definition defmixture, name = "Mixture distribution und zero-inflated distribution"}
\begin{equation*}
  P(Y = y) = p \cdot g_1(y) + (1-p) \cdot g_2(y)
\end{equation*}

mit $p$ als *mixture proportion* und $g_1, g_2$ als Dichte-/Massefunktionen der beiden Komponenten.

Für zero-inflation setzt man $p$ als *rate of zero-inflation* $\pi$, $g_1$ als degenerierten 0-Verteilung und $g_2$ als Poisson-PMF oder ähnliche Zähl-Verteilung wie NB, PIG, etc.  

Eine alternative Darstellung für eine *zero-inflated* Verteilung ([siehe](https://www.statistik.uni-dortmund.de/useR-2008/slides/Kleiber+Zeileis.pdf)):

\begin{equation*}
f_\mathrm{zeroinfl} (y; x, \beta, \pi) = \pi \cdot I_{\{0\}}(y) + (1 - \pi) \cdot f_{\mathrm{count}}(y; x, \beta)
\end{equation*}

Mit zu modellierendem Erwartungswert

\begin{equation*}
\mu_i = \pi_i \cdot 0 + (1 - \pi_i) \cdot \exp(x_i^T\beta)
\end{equation*}

Wobei $\pi$ in der Regel binomial, bzw. mittels logistischer Regression geschätzt wird.
```

Und im Speziellen, anhand der Beispiele ZIP und ZINB:

```{definition defzip, name = "Zero-Inflated Poisson Verteilung (ZIP)"}
Nach @perumean-chaneyZeroinflatedOverdispersedWhat2013 (p. 1675)

\begin{align*}
P(Y = 0) &= \pi + (1-\pi) \cdot e^{-\mu} \\
P(Y = y) &= (1 - \pi) \cdot \frac{\mu^y e^{-\mu}}{y!}, \quad y = 1, 2, 3, \ldots
\end{align*}
```

```{definition defzinb, name = "Zero-Inflated Negative Binomialverteilung (ZINB)"}
Nach @perumean-chaneyZeroinflatedOverdispersedWhat2013 (p. 1675)

\begin{align*}
P(Y = 0) &= \pi + (1 - \pi) \cdot \frac{\theta^\theta}{(\theta + \mu)^\theta}  \\
P(Y = y) &= (1 - \pi) \cdot \frac{\Gamma(\theta+y)}{\Gamma(\theta) \Gamma(y + 1)} \frac{\theta^\theta \mu^y}{(\theta + \mu)^{(\theta + y)}}, \quad y = 1, 2, 3, \ldots
\end{align*}
```

### Interpretation

Ein Anwendungsbeispiel der ZIP findet sich auf der Seite der UCLA ([SAS](https://stats.idre.ucla.edu/sas/dae/zero-inflatedpoisson-regression/), [R](https://stats.idre.ucla.edu/r/dae/zip/), zusätzlich gibt es einmal [annotated Output (SAS)](https://stats.idre.ucla.edu/sas/output/zero-inflated-poisson-regression/)), wobei der `fish`-Datensatz verwendet wird.

In diesem Datensatz beobachten wir Nullen mit zwei unterschiedlichen Ursprüngen: Zum Einen haben wir die Nullen der Personen, die geangelt haben, aber keine Fische fingen – und zum Anderen haben wir die Nullen der Personen, die *nicht angelten*, und daher garantiert auch keine Fische fingen ("*certain zeros*") (vgl. auch Abschnitt \@ref(zi-modelling)).  

Ein ZIP-Modell erlaubt uns hier also beide Komponenten zu berücksichtigen – sowohl die Personen, die erfolglos angelten, als auch die Personen, die gar nicht angelten.

```{r zip-fish}
# ZP basierend auf fish-Daten, Modell aus UCLA-Tutorialseite
m_zip <- pscl::zeroinfl(count ~ child + camper | persons, data = fish)

summary(m_zip)

# Exponentierte Koeffizienten
tibble::enframe(exp(coef(m_zip)), name = "coefficient")
```

Wir erhalten zwei Sets an Koeffizienten:

**Count-model**: Das count model (`count_`), i.e. Poisson. Hier modellieren wir die Anzahl der gefangen Fische ohne Berücksichtigung der *excess zeros*. Diese Koeffizienten werden analog einer herkömmlichen Poisson-Regression interpretiert.

**Inflation-model**: Die binomiale Komponente (`zeros_`) für die *excess zeros*, womit wir die Wahrscheinlichkeit von "certain zeros" modellieren. Diese Koeffizienten werden analog einer logistischen Regression interpretiert.  
Aus dem UCLA DAE:

> **persons**: If a group were to increase its persons value by one, **the odds that it would be in the "Certain Zero" group** would decrease by a factor of exp(-0.5643) = 0.5687581. In other words, the more people in a group, the less likely the group is a certain zero.

> **Intercept**: If all of the predictor variables in the model are evaluated at zero, the odds of being a "Certain Zero" is exp(1.2974) = 3.659769.  This means that the predicted odds of a group with zero persons is  3.659769 (though remember that evaluating persons at zero is out of the range of plausible values–every group must have at least one person). 


```{r zi-principle-plot}
nsim <- 10^4
meansim <- 2

tibble::tibble(
  counts = rpois(n = nsim, lambda = meansim),
) %>%
  count(counts, name = "Poisson") %>%
  mutate(
    Zeros = floor(Poisson/2),
    Zeros = ifelse(counts > 0, 0, Zeros)
  ) %>%
  gather(source, freq, Poisson, Zeros) %>%
  mutate(source = forcats::fct_rev(source),
         freq = freq/nsim) %>%
  ggplot(aes(x = counts, y = freq, fill = source)) +
  geom_col(alpha = .75) +
  scale_x_continuous(breaks = seq(0, 12)) +
  scale_fill_brewer(palette = "Set1", labels = c(Zeros = "Certain Zeros", Poisson = "Poisson-Modell")) +
  labs(
    title = "Schaubild: Zero-Inflated Poisson",
    subtitle = "Nullen in den Daten: Certain zeros + count zeros",
    x = "Count", y = "Anteil an Gesamtsichtprobe", fill = "",
    caption = "Simulierte Daten"
  ) +
  theme(legend.position = "bottom")
```

