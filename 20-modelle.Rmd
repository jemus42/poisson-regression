---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Mehrparametrische Modelle {#multiparam}

```{r model-graph, echo=FALSE, cache=FALSE, fig.cap="Hierarchie ausgewählter Count-Modelle. In Klammern: Zu schätzende Parameter der zugrundeliegenden Verteilung"}
grViz(readLines("graphs/model-graph.dot"))
```


Zweiparametrische Modelle haben neben dem Parameter für den Erwartungswert einen weiteren Parameter für die Dispersion, was wir natürlich insbesondere im Kontext der Dispersionsproblematik sehr nützlich finden.  
Alle hier aufgeführten Modelle (inklusive der zero-inflation models) können als Verallgemeinerung der Poisson-Verteilung um (mindestens) einen weiteren Parameter aufgefasst werden. Die Unterschiede liegen hauptsächlich in der Parametrisierung und den damit zusammenhängenden Einschränkungen. Die *Negative Binomialverteilung* und die *Poisson Inverse Gaussian* zum Beispiel erweitern beide die Poisson um einen Dispersionsparameter, machen aber unterschiedliche Annahmen zur Verteilung der Varianz (gamma- vs. invers-normalverteilt).  

Tabelle \@ref(tab:modelstable) zeigt eine Reihe von Verteilungen mit entsprechend parametrisierter Varianz abhängig vom Erwartungswert $\mu$ und einem Dispersionsparameter $\alpha$.

```{r modelstable, echo=FALSE}
tibble::tribble(
  ~Model,                         ~Mean,  ~Variance,
  "Poisson",                     "$\\mu$", "$\\mu$",
  "Negative Binomial I (NB1)",   "$\\mu$", "$\\mu (1 + \\alpha) = \\mu + \\alpha\\mu$",
  "Negative Binomial II (NB2)",  "$\\mu$", "$\\mu (1 + \\alpha\\mu) = \\mu + \\alpha\\mu^2$",
  "Negative Binomial-P (NBP)",   "$\\mu$", "$\\mu (1 + \\alpha\\mu^p) = \\mu + \\alpha\\mu^p$",
  "Poisson Inverse Gaussian (PIG)", "$\\mu$", "$\\mu (1 + \\alpha\\mu^2) = \\mu + \\alpha\\mu^3$",
  "Generalized Poisson (GP)",    "$\\mu$", "$\\mu (1 + \\alpha\\mu)^2 = \\mu + 2\\alpha\\mu^3 + \\alpha^2\\mu^3$"
 ) %>%
  kable(booktabs = TRUE, escape = FALSE, linesep = "",
        caption = " Ein Überblick einiger Modelle mit Varianzparametrisierung (nach @hilbeModelingCountData2014, p. 12).") %>%
  kable_styling(position = "center", protect_latex = FALSE)
```


## Negativ Binomial (NB) {#mod-nb}

Die Negative Binomialverteilung (*NB*) kann als zweiparametrische Erweiterung der *Poisson* betrachtet werden, wobei die Varianz (separat vom Erwartungswert) als Gamma-verteilte Zufallsvariable betrachtet wird (*Poisson-Gamma Mischmodell*).  
Für eine ausführliche Beschreibung, siehe @hilbeModelingCountData2014 (p. 126ff).

```{definition defNB, name = "Negative Binomialverteilung"}
Nach @perumean-chaneyZeroinflatedOverdispersedWhat2013 (p. 1675):

Für $Y \sim NB(\mu, \theta)$ gilt
  
\begin{equation}
P(Y = y) = \frac{\Gamma(\theta + y)}{\Gamma(\theta) \Gamma(y+1)} \frac{\theta^\theta \mu^y}{(\theta + \mu)^{(\theta + y)}}, \quad y = 0, 1, 2, \ldots
\end{equation}

Mit $\mathbb{E}(Y) = \mu$ und $\mathrm{Var}(Y) = \mu + \frac{\mu^2}{\theta}$
```

Für eine alternative Darstellung unter Verwendung von $\alpha = \frac{1}{\theta}$, siehe @hilbeModelingCountData2014 (p. 130)

Der gängigste Anwendungsfall der *NB* findet sich bei Daten mit nicht korrigierbarer overdispersion (siehe Abschnitt \@ref(overdispersion)), da der Parameter $\theta$ bzw. auch $\alpha = \frac 1 \theta$ als *Dispersionsparameter* dient:

```{definition nbdisppar, name = "NB-Dispersionsparameter"}

Meist wird der Parameter als $\alpha$ bezeichnet, mit der Interpretation (relativ zum Poisson-Modell):
  
$$\mathrm{Var}(Y) = \mu + \alpha \mu^2$$

\begin{align}
  \alpha &= 0 &&\Longrightarrow \text{ Äquivalent zur Poissonverteilung} \\
  \alpha &> 0 &&\Longrightarrow \text{ Overdispersion}
\end{align}

Je nach Quelle (und unter Anderem in R (z.B. `MASS::glm.nb`)) wird die inverse Variante $\theta = \frac 1 \alpha$ verwendet, womit gilt:

$$\mathrm{Var}(Y) = \mu + \frac{\mu^2}{\theta}$$

\begin{align}
  \theta &> 0 &&\Longrightarrow \text{ Overdispersion} \\
  \theta &\to \infty &&\Longrightarrow \text{ Äquivalent zur Poissonverteilung}
\end{align}

Siehe dazu auch @hilbeModelingCountData2014 (p. 131).

```

Das GLM sieht keinen weiteren zu schätzenden Parameter vor, weshalb $\theta$ bzw. $\alpha$ in der Praxis separat geschätzt wird.  
Der wohl wichtigste Aspekt des Dispersionsparameters, unabhängig davon ob $\alpha$ oder $\theta$ verwendet wird, ist sein Vorzeichen: Er ist _immer_ positiv, das heißt die Varianz der NB ist entweder _größer oder gleich_ der Poisson, oder mit anderen Worten:

> "The negative binomial model adjusts for Poisson overdispersion; it **cannot be used to model underdispersed** Poisson data" 
>
> --- [@hilbeModelingCountData2014 (p. 11), eigene Hervorhebung]

Je nach Software/Algorithmus ist es auch möglich, dass ein NB-fit nicht möglich ist, wenn $\alpha \approx 0$ (die Verteilung zu nah an Poisson) bzw. die Daten Poisson-underdispersed sind. 

Weiterhin gibt es zwei verschiedene Formulierungen der Negativen Binomialverteilung, NB1 und NB2, deren Nummerierung auf den Exponenten im zweiten Term ihrer Varianzen zurückzuführen ist:

```{definition defNB1NB2, name = "NB1 und NB2"}
Nach @hilbeModelingCountData2014 (p. 126f):

Die **NB1**, oder auch *lineare* Negative Binomialverteilung hat die Varianz

\begin{equation*}
\mathrm{Var}(Y) = \mu + \alpha \mu
\end{equation*}

…und die gängigere **NB2**, oder auch *quadratische* Negative Binomialverteilung hat (wie oben beschrieben) die Varianz

\begin{equation*}
\mathrm{Var}(Y) = \mu + \alpha \mu^2
\end{equation*}

```

Beide Varianten können wie MLE geschätzt werden, wobei NB2 zusätzlich im Kontext des GLM via IRLS geschätzt werden kann.

Als link wird $\log \mu$ verwendet, wobei der *canonical link* eigentlich $\log\left(\frac{\alpha\mu}{1 + \alpha\mu}\right)$ ist [^nblink]. 

[^nblink]: Daher können Standardfehler nicht wie für canonical models überlich via OIM (*observed information matrix*), sondern via EIM (*expected information matrix*) geschätzt werden. Die EIM-Methode liefert weniger genaue Resultate für kleine Stichproben (n < 30).

Für ein gut angepasstes NB-Modell gilt analog eines Poisson-Modells, dass die Dispersionsstatistik approximativ 1 ist (siehe \@ref(def:def-pearsondisp)).

Zur Anwendung in R gibt es zwei Möglichkeiten: `MASS::glm.nb` oder `msme::nbinomial`, wobei letztere mehrere Optionen zur Parametrisierung hat und zusätzlich die Dispersionsstatistik ausgibt:

```{r nb_r, warning=FALSE}
mod_nb <- MASS::glm.nb(docvis ~ outwork + age,
                       data = rwm1984)
summary(mod_nb)


mod_nb2 <- msme::nbinomial(docvis ~ outwork + age,
                           data = rwm1984)
summary(mod_nb2)
```


## Poisson Inverse Gaussian (PIG) {#mod-pig}

So wie die *NB* als mixture aus Poisson-Verteilung mit Gamma-verteilter Varianz betrachtet werden kann, ist die *Poisson Inverse Gaussian* (*PIG*) eine mixture aus Poisson-Verteilung mit [invers-normalverteilter](https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution) Varianz. Demnach kann man *PIG* als Alternative zur *NB* verwenden, insbesondere für den Fall sehr rechtsschiefer Daten.


> "Simply put, PIG models can better deal with highly overdispersed data that can negative binomial regression, particularly data clumped heavily at 1 and 2"
> 
> -- @hilbeModelingCountData2014 (p. 163)

**Software**:

- R: `gamlss` (CRAN)
- SAS: Machbar via `NLMIXED`, aber unschön [siehe @high2018AlternativeVariance].
- Stata: `pigreg`

## Generalized Poisson (GP) {#mod-gp}

Die Verteilung besitzt auch einen zusätzlichen Dispersionsparameter (analog NB, PIG), allerdings kann dieser hier auch negative Werte annehmen, womit die GP für alle Dispersionsarten (bzw. insbesondere underdispersion) geeignet ist.

```{definition def-genois, name = "Generalized Poisson"}
Aus @hilbeModelingCountData2014 (p. 211) und @harris2012ModelingUnderdispersed:

Eine Variable $Y$ ist *generalized Poisson* verteilt mit der PMF:
  
\begin{equation*}
P(Y = y_i) = \frac{\theta_i \left(\theta_i + \delta y_i\right)^{y_i - 1}}{y_i !} \exp(-\theta_i - \delta y_i), \quad y_i = 1, 2, 3, \ldots
\end{equation*}

\begin{align*}
\theta_i > 0, \quad \max(-1, \frac{-\theta_i}{4}) < \delta < 1 \\[2em]
\mu_i = \mathbb{E}(Y_i) &= \frac{\theta_i}{1 - \delta} \\
\mathrm{Var}(Y_i) &= \frac{\theta}{(1 - \delta)^3} 
                   = \frac{1}{(1 - \delta)^2} \mathbb{E}(Y_i)
                   = \Phi \mathbb{E}(Y_i)
\end{align*}

Wobei $\Phi = \frac{1}{(1 - \theta)^2}$ als Dispersionsparameter dient.

Es gilt außerdem

\begin{align*}
\delta = 0 &\Longrightarrow \text{Equidispersion (Poisson)} \\
\delta < 0 &\Longrightarrow \text{Underdispersion} \\
\delta > 0 &\Longrightarrow \text{Overdispersion}
\end{align*}

Auch hier kann ein Likelihood-Ratio Test auf $\delta = 0$ durchgeführt werden, analog $\alpha = 0$ für die NB – wobei $\delta$ hier keine Restriktion hat (siehe BLR, Definition \@ref(def:BLR)).

```


**Software**:  

- R: `VGAM`: `vgam(..., family = genpoisson())`]
    - Hier wird das obige $\delta$ als $\lambda$ bezeichnet (Verwechslungsgefahr mit Poisson-Parameter)
    - Numerisch instabil für $\lambda$ nahe 0 oder 1
    - Siehe [`?VGAM::genpoisson`](https://rdrr.io/cran/VGAM/man/genpoisson.html)
- Alternativ via [gamlss](https://www.gamlss.com/): `gamlss(..., family = GPO)`

In beiden R-Varianten habe ich bisher nicht geschafft $\theta$, den Dispersionsparameter, zu extrahieren – auch wenn die Koeffizientien aus dem Stata-Beispiel in [@hilbeModelingCountData2014, p. 213f] reproduzierbar sind.

- SAS: `NLMIXED` oder `FMM`, siehe auch [SAS support document](http://support.sas.com/kb/56/549.html)

Allgemein scheint die Parametrisierung (und Notation der Parameter) zu variieren.


## Conway-Maxwell Poisson (COMP) {#mod-comp}

```{definition, name = "Conway-Maxwell-Poisson Modell (CMP)"}
Nach @shmueli2005UsefulDistribution (p. 129):
  
\begin{align}
P(X = x) &= \frac{\lambda^x}{(x!)^\nu} \frac{1}{Z(\lambda, \nu)}, \quad x = 0, 1, 2, \ldots \\
Z(\lambda, \nu) &= \sum_{j = 0}^\infty \frac{\lambda^j}{(j!)^\nu} \\
\\
\lambda > 0&, \nu \ge 0
\end{align}

Mit (im Unterschied zur Poisson) nichtlinearer Zerfallsrate $\nu$ *rate of decay*, so dass:

\begin{equation}  
\frac{P(X = x -1)}{P(X = x)} = \frac{x^\nu}{\lambda}
\end{equation}

Der Zusammenhang zwischen $\nu$ und $\lambda$ nach @sellers2010FlexibleRegression (p. 946) legt Nahe, dass $\nu$ ähnlich $\alpha$ in NB-Modellen die Dispersion bestimmt:
  
\begin{align*}
  \mathrm{Var}(Y_i) =& \lambda_i \frac{\partial}{\partial \lambda_i} \mathbb{E}(Y_i)
            \approx \lambda_i \frac{\partial}{\partial \lambda_i} \left( \lambda_i^{\frac{1}{\nu}} - \frac{\nu - 1}{2 \nu} \right) \\
                  =& \frac{1}{\nu} \lambda^{\frac{1}{\nu}}
           \approx \frac{1}{\nu} \mathbb{E}(Y_i)
\end{align*}

Als link wird $\log \lambda$ verwendet, da so die links für Poisson- und logistische Regression Spezialfälle der COMP sind.
```

Die Verteilung hat als Spezialfälle:

- $\nu = 1 \Longrightarrow Z(\lambda, \nu) = exp(\lambda)$: Poisson
- $\nu \to \infty \Longrightarrow Z(\lambda, \nu) \to 1 + \lambda$: Bernoulli mit $P(X = 1) = \frac{\lambda}{1+\lambda}$
- $\nu = 0,\ \lambda < 1$: Geometrisch mit $P(X = x) = \lambda^x (1 - \lambda)$


**Vorteile**:

- "Brücke" zwischen logistischer und Poisson-Regression [@sellers2010FlexibleRegression]

> Although the logistic regression is a limiting case $(\nu \to \infty)$, in practice, fitting a COM-Poisson regression to binary data yields estimates and predictions that are practically identical to those from a logistic regression.
>
> --- @sellers2010FlexibleRegression, p. 945

- "Low cost", i.e. "nur" ein zusätzlicher Parameter
- Einfach zu handhaben (wenn in GLM mit MLE statt MCMC estimation)
- Over- und underdispersion abbildbar
- Zero-inflation abbildbar

**Software**:

- `CompGLM::glm.comp`: Funktioniert, aber crasht RStudio wenn `nuFormula` spezifiziert wird
- `compoisson::com.fit`: Entweder generell nicht geeignet oder unfassbar langsam (und deshalb nicht geeignet)
- `COMPoissonReg` (GitHub: `lotze/COMPoissonReg`): Scheint zu funktionieren

Da die COMP-Wahrscheinlichkeitsfunktion eine unendliche Summe $Z(\lambda, \nu)$ enthält, kann es bei der Anwendung in Software durchaus dazu kommen, dass keine Konvergenz erreicht wird.  Wie viele Iterationen für die Konvergenz der Summe benötigt werden hängt zum einen von $\nu$ ab: Je größer, desto schneller die Konvergenz. Das Gegenteil gilt für $\lambda$ – je größer, desto langsamer die Konvergenz [@high2018AlternativeVariance, p. 4]. 

Siehe auch

- @sellers2010FlexibleRegression für eine gute Übersicht
- @lord2010ExtensionApplication, @lord2008ApplicationConwayMaxwellPoisson für eine Anwendung
- @shmueli2005UsefulDistribution


# Weitere Modelle

## Diagnostische Modelle


### Heterogenous Negative Binomial (NBH) {#mod-nbh}

- Selling feature: Erlaubt Parametrisierung von $\alpha$
- -> Ursache für under-/overdispersion modellierbar

@hilbeModelingCountData2014 (p. 156)

```{r nbh, warning=FALSE}
data(nuts, package = "COUNT")
nuts <- subset(nuts, dbh < 0.6)

ggplot(data = nuts, aes(x = cones)) +
  geom_histogram(binwidth = 1)

# Poisson-Modell als Startpunkt
m_pois <- glm(cones ~ sntrees + sheight + scover, family = poisson(), data = nuts) 
dispersion(m_pois) # -> overdispersion 

# NB-Modell
m_nb <- msme::nbinomial(cones ~ sntrees + sheight + scover, data = nuts)
summary(m_nb) # Dispersion < 1 -> NB-underdispersed

# NB-H: Ursache der Dispersion?
m_nbh <- msme::nbinomial(cones ~ sntrees + sheight + scover, 
                         formula2 = cones ~ sntrees + sheight + scover,
                         scale.link = "log_s",
                         family = "negBinomial",
                         data = nuts)
summary(m_nbh)
```

Signifikanz der Dispersionsprädiktoren (Suffix `_s`) kann nun verwendet werden, um Ursachen der overdispersion zu identifizieren – auch wenn in diesem Beispiel keiner der Koeffizienten signifikant wird.

### Negative binomial-P (NB-P) {#mod-nbp}

Eine Erweiterung der NB1 und NB2, die den Exponenten im Varianzterm parametrisiert ($\rho$), womit die Dispersion nun nicht mehr statisch für alle Beobachtungen ist:

```{definition defnbp, name = "NB-P"}
\begin{align*}
  \text{NB-P: }\qquad \mathrm{Var}(Y) &= \mu + \alpha\mu^\rho \\
  \text{NB1: } \qquad \mathrm{Var}(Y) &= \mu + \alpha\mu      \\
  \text{NB2: } \qquad \mathrm{Var}(Y) &= \mu + \alpha\mu^2
\end{align*}
```

Damit ist NB-P äquivalent zu NB2 für $\rho = 2$ bzw. NB1 für $\rho = 1$.  
Um zu evaluieren, ob ein NB1 oder NB2 Modell einen besseren Fit liefert, kann man nun ein NB-P Modell zum Vergleich fitten und Likelihood Ratio Tests zum Vergleich heranziehen.

Siehe auch @hilbeModelingCountData2014 (p. 155) – wo der Autor zwar den obigen Ansatz beschreibt, aber zur Entscheidung zwischen NB1/2 zusätzlich auf die sowieso etablierten Informationskriteren (AIC/BIC) zurückgreift. Dazu gehört auch, dass das NB-P kein rein diagnostisches Werkzeug sein muss: Wenn ein NB-P ein _deutlich_ kleineres AIC aufweist als NB1/2-Modelle, kann das NB-P auch als Modell der Wahl in Erwägung gezogen werden.

## Zero-Inflated Models (*mixture models*) {#mod-zi}

Zero-inflated Modelle sind *mixtures*, das heißt, sie entstehen durch Mischung zweier Wahrscheinlichkeitsfunktionen. In der Regel wird eine Nullverteilung mit einer regulären Zählverteilung (Poisson, NB, PIG, ...) "gemischt", abhängig von einem *mixture parameter* $\pi \in [0,1]$, der z.B. logistisch modelliert wird. Die Nullen im Modell werden dementsprechend durch beide Verteilungen (Nullverteilung, Zählverteilung) beschrieben – im Gegensatz zu *hurdle models* (vgl. \@ref(mod-hurdle)), die Nullen logistisch und die positiven counts über eine *truncated* distribution modellieren würden.  

Zero-inflated distributions wie ZIP und ZINB wurden hergeleitet als zweiteilige mixture distributions. Die allgemeine Form für mixture distributions ist

```{definition defmixture, name = "Mixture distribution und zero-inflated distribution"}
\begin{equation*}
  P(Y = y) = p \cdot g_1(y) + (1-p) \cdot g_2(y)
\end{equation*}

mit $p$ als *mixture proportion* und $g_1, g_2$ als Dichte-/Massefunktionen der beiden Komponenten.

Für zero-inflation setzt man $p$ als *rate of zero-inflation* $\pi$, $g_1$ als degenerierten 0-Verteilung und $g_2$ als Poisson-PMF oder ähnliche Zähl-Verteilung wie NB, PIG, etc.  

Eine alternative Darstellung für eine *zero-inflated* Verteilung ([siehe](https://www.statistik.uni-dortmund.de/useR-2008/slides/Kleiber+Zeileis.pdf)):

\begin{equation*}
f_\mathrm{zeroinfl} (y; x, \beta, \pi) = \pi \cdot I_{\{0\}}(y) + (1 - \pi) \cdot f_{\mathrm{count}}(y; x, \beta)
\end{equation*}

Mit zu modellierendem Erwartungswert

\begin{equation*}
\mu_i = \pi_i \cdot 0 + (1 - \pi_i) \cdot \exp(x_i^T\beta)
\end{equation*}

Wobei $\pi$ in der Regel binomial, bzw. mittels logistischer Regression geschätzt wird.
```


```{definition defzip, name = "Zero-Inflated Poisson Verteilung (ZIP)"}
Nach @perumean-chaneyZeroinflatedOverdispersedWhat2013 (p. 1675)

\begin{align*}
P(Y = 0) &= \pi + (1-\pi) \cdot e^{-\mu} \\
P(Y = y) &= (1 - \pi) \cdot \frac{\mu^y e^{-\mu}}{y!}, \quad y = 1, 2, 3, \ldots
\end{align*}
```

```{definition defzinb, name = "Zero-Inflated Negative Binomialverteilung (ZINB)"}
Nach @perumean-chaneyZeroinflatedOverdispersedWhat2013 (p. 1675)

\begin{align*}
P(Y = 0) &= \pi + (1 - \pi) \cdot \frac{\theta^\theta}{(\theta + \mu)^\theta}  \\
P(Y = y) &= (1 - \pi) \cdot \frac{\Gamma(\theta+y)}{\Gamma(\theta) \Gamma(y + 1)} \frac{\theta^\theta \mu^y}{(\theta + \mu)^{(\theta + y)}}, \quad y = 1, 2, 3, \ldots
\end{align*}
```
