---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Voraussetzungen und Einschränkungen

Es gelten Standardannahmen des GLM:

- Die abhängige Variable $Y$ kommt aus der Exponentialfamilie (Normal-, Poisson-, Gamma-, Binomialverteilung)
- Modelliert wird der lineare Prädiktor $\eta_i = \boldsymbol{x}_i' \boldsymbol{\beta}$.
- Response- und Linkfunktionen $h(x)$ und $g(x) = h^{-1}(x)$ 



### Overdispersion {#overdispersion}

Der vermutlich häufigste Fall für Count-Daten: Die Varianz der abhängigen Variable ist größer als ihr Erwartungswert, bzw. größer als ihre erwartete Varianz innerhalb eines Modells. 
@hilbeModelingCountData2014 unterscheidet zwischen echter und scheinbarer (*apparent*) Overdispersion, wobei letztere oft durch geeignete Korrekturen kompensiert werden kann, wobei *echte* Overdisperion sowohl Parameterschätzung und Modellanpassung im Allgemeinen beeinträchtigt.

Nach @hilbeModelingCountData2014 (p. 82) entsteht *echte* Overdispersion durch:

- Positive Korrelation zwischen responses
- Große Variation zwischen response-probabilities und counts
- Verletzungen der Verteilungsannahme (i.e. Poissonverteilung)
- "Proneness": Frühere Ereignisse beeinflussen das Auftreten späterer Ereignisse [^poiind]

[^poiind]: Diese Annahme (Unabhängigkeit der Ereignisse) der Poissonverteilung ist auch der Grund, warum sich die Poissonverteilung prinzipiell **nicht** eignet um Epidemien wie Ebola zu modellieren. Freundliche Grüße an Frau Pigeot, "Statistische Modellierung I", WiSe 18/19.


Ursachen für *scheinbare* (*apparent*), und damit (bedingt) korrigierbare Overdispersion nach @hilbeModelingCountData2014, p. 41, 82

1. Fehlende explanatorische Prädiktoren
2. Ausreißer
3. Fehlende Interaktionsterme
4. Ein Prädiktor muss transformiert werden
5. Die Daten sind zu dünn besetzt (*sparse*)
6. Fehlende Werte, die nicht zufällig sind (missing not at random, *MNAR*) (siehe dazu auch Anhang \@ref(appendix-missingness))

Ein einfaches simuliertes Beispiel zur Auswirkung von fehlenden Prädiktoren:

```{r overdisp_misspecified_model_sim}
# Generate binary variable in [0, 1] with a given proportion of 1's
rbinary <- function(n, prob = 0.5) {
  sample(0:1, size = n, replace = TRUE, prob = c(1 - prob, prob))
}

set.seed(436)
n <- 1000

sim <- tibble(
  x1 = rbinary(n, .1),
  x2 = rbinary(n, .2),
  x3 = rbinary(n, .3),
  eta = 0.5 + 1 * x1 + 2 * x2 + 0.5 * x3,
  lambda = exp(eta),
  py = rpois(n, lambda)
)

# Korrektes modell:
mod <- glm(py ~ x1 + x2 + x3, data = sim, family = poisson(link = "log"))
dispersion(mod)

# Modell mit fehlendem Prädiktor:
mod2 <- glm(py ~ x1 + x3, data = sim, family = poisson(link = "log"))
dispersion(mod2)
```

#### Diagnostik {#overdisp-diag}

Der *Score-Test* wird nicht unbedingt empfohlen, findet sich aber in der Literatur:

```{definition, name = "Score-Test auf Overdispersion"}
Nach @hilbeModelingCountData2014 (p. 85f):
  
Sei $Y$ die abhängige Variable und $y_i$ die $i-te$ Beobachtung, mit $\hat{\mu}_i$ als gefitteter Wert eines Poisson-Modells, dann wird der z-Score wie folgt berechnet:

\begin{equation}
  z_i = \frac{(y_i - \hat{\mu})^2 -y}{\hat{\mu} \sqrt{2}}
\end{equation}
  
Zur Evaluation von $z$ wird z.B. ein Intercept-only Regressionsmodell mit $z$ als abhängiger Variable gefitted, mit der Nullhypothese "Es liegt keine Overdispersion vor".

Dabei gelten folgende Annahmen:
  
1. Die Stichprobe ist *groß*
2. $z$ ist t-verteilt

```



```{definition, name = "Lagrange Multiplier Test"}
Nach @hilbeModelingCountData2014 (p. 85f):

Eine $\chi^2$-Teststatistik mit einem Freiheitsgrad.
  
\begin{equation}
  \chi^2 = \frac{\left( \sum_{i=1}^n \hat{\mu}_i^2 - n \bar{y} \right)^2}
                {2 \sum_{i=1}^n \hat{\mu}_i^2}
\end{equation}

```

Eine rudimentäre R-Implementation findet sich in Abschnitt \@ref(helperfuns).  

#### Diagnostik: Beispiel 

Als Beispiel verwenden wir hier ein Subset des Datensatz `rwm5yr` (vgl. Abschnitt \@ref(data), `?COUNT::rwm5yr`), der Angaben zur Anzahl der Arztbesuche pro Person mit zusätzlichen demographischen Merkmalen enthält. Für unser Beispielmodell verwenden wir folgende Variablen:

- `docvis`: Abhängige Variable, Anzahl der Arztbesuche im Jahr (0-121)
- `outwork`: Arbeitslos (1), arbeitend (0)
- `age`: Alter (25 - 64)

```{r overdispersion-example}
# Daten + subsetting
data(rwm5yr, package = "COUNT")
rwm1984 <- subset(rwm5yr, year == 1984)

# Model fit
mod <- glm(docvis ~ outwork + age, family = poisson(), data = rwm1984)

# Pearson dispersion
dispersion(mod)

# Lagrange-Test
lagrange_test(mod)
```

Sowohl die Pearson-Dispersion als auch Lagrange sind sich einig, dass wir es hier mit overdispersion zu tun haben.

Zusätzlich können wir die beobachteten und auf Basis des Modells erwarteten Häufigkeiten vergleichen, um ein grobes Gefühl für die Situation zu erhalten (Code frei adaptiert nach @hilbeModelingCountData2014, p. 88f):

```{r overdisp_compare_expected}
# Beobachtete und erwartete counts
observed_v_expected <- rwm1984 %>%
  count(docvis, name = "observed") %>%
  mutate(
    expected = purrr::map_dbl(docvis, ~{
                  dpois(.x, fitted(mod)) %>%
                    sum() %>%
                    round(2)
                }),
    difference = observed - expected
  ) 

observed_v_expected %>%
  head(5) %>%
  pander(caption = "Observed and expected counts")

# Mittelwert und Varianz der jeweiligen counts
# (für "expected" gilt Varianz := Mittelwert)
tribble(
  ~Counts,   ~Mean,                 ~Variance,
  "observed", mean(rwm1984$docvis), var(rwm1984$docvis),
  "expected", mean(fitted(mod)),    mean(fitted(mod))
) %>%
  pander(caption = "Mean & variance of observed and expected counts")

# Plot: observed vs. expected counts
observed_v_expected %>%
  filter(docvis <= 12) %>%
  gather(type, count, observed, expected) %>%
  ggplot(aes(x = docvis, y = count, fill = type, color = type)) +
  geom_point(shape = 21, color = "black", stroke = .5, size = 2) +
  geom_path(linetype = "dotted", size = .25) +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_fill_brewer(palette = "Set1", aesthetics = c("color", "fill"), name = "") +
  labs(
    title = "rwm1984: Poisson Modell",
    subtitle = "Observed und expected counts bei overdispersion",
    caption = "Output begrenzt auf docvs <= 12",
    x = "Anzahl der Arztbesuche (docvis)", y = "Count"
  ) +
  theme(legend.position = "top")
```


Anhand der ersten Tabelle können wir recht schnell erkennen, dass wir hier *deutlich* mehr Nullen beobachten als das Modell vorhersagt – mehr dazu in Abschnitt \@ref(zeros).  
Der Plot veranschaulicht den eher suboptimalen model fit unter diesen Umständen (overdispersion & zero-inflation).


#### Umgang mit Overdispersion

Zur expliziten Modellierung des Dispersionparameters kann ein NBH Modell (\@ref(mod-nbh)) genutzt werden, falls die Quelle der overdispersion von besonderem Interesse ist.  

Abseits davon bleiben 2 grobe Strategien : 

1. Adjustierung der durch die overdispersion verzerrten Standardfehler (Quasipoisson, Quasi-Likelihood, robuste Varianzschätzer)
2. Wechsel auf ein Modell, das overdispersion (oder allgemeine extradispersion) erlaubt (e.g. NB)

(Bemerke: Standardfehler für IRRs werden i.A. über die *delta method* bestimmt, die ich noch recherchieren und irgendwo einbauen muss)

##### Quasipoisson: Skalierung der Standardfehler {-}

Hier werden lediglich die Standardfehler der Koeffizienten eines Poisson-Modells adjustiert, die bei overdispersion i.d.R. unterschätzt werden – die eigentliche Parameterschätzung wird nicht beeinflusst:

$$\mathrm{SE}(\beta_k) \cdot \sqrt{D}$$
Wobei $D$ der Pearson-Dispersionsindex ist (vgl. @hilbeModelingCountData2014, p. 92ff), den wir in \@ref(def:pearsondisp) als $\frac{\chi^2_{\mathrm{Pearson}}}{\mathrm{df}}$ definiert hatten.

Ein möglicher Nachteil jedoch: Es wird zuerst ein reguläres Poisson-Modell gefittet, Standardfehler und Dispersionsindex bestimmt, und dann dasselbe Modell mit skalierten Standardfehlern erneut gefittet – dementsprechend ist diese Methode vermutlich für größere Datensätze eher ineffizient.

Einmal auf unser voriges Beispiel anhand der `rwm5yr`-Daten angewandt:

```{r quasipois-example}
# Urprüngliches Model
mod <- glm(docvis ~ outwork + age, family = poisson(), data = rwm1984)

mod_qp <- glm(docvis ~ outwork + age, family = quasipoisson(), data = rwm1984)

dispersion(mod)
dispersion(mod_qp)
```

An der Dispersion hat sich nichts geändert, nur an den Standardfehlern der Koeffizienten (in folgenden Tabellen auf log-Skala):

```{r quasipois-example2}
pander(mod, caption = "Poisson-Modell")
pander(mod_qp, caption = "Quasipoisson-Modell")
```

In diesem Fall "lohnt" sich dieser Ansatz zwar eher nicht, da wir neben overdispersion noch ein Problem mit zero-inflation haben, aber in manchen Fällen kann Quasipoisson-Methode ausreichen.

> Scaling is an easy, quick-and-dirty method of adjusting standard errors for overdispersion. However, when data are highly correlated or clustered, model slopes or coefficients usually need to be adjusted as well. Scaling does not accommodate that need, but simulations demonstrate that it is quite useful for models with little to moderate overdispersion.
>
> --- @hilbeModelingCountData2014, p. 96

##### Robuste Varianzschätzer (Sandwich Estimators) {-}

(Andere Namen: Huber & White standard errors, empirical standard errors)

Sandwich estimators sind in erster Linie für nicht unabhängige bzw. korrellierte Daten gedacht, zum Beispiel für Daten, die innerhalb mehrere Haushalte, Krankenhäuser, Städte etc. gesammelt wurden.

Robuste Varianzschätzer können (und je nach Quelle: *sollten*) standardmäßig für Count-response Daten verwendet werden, da die resultierenden Schätzer im Falle tatsächlich unkorrellierter Daten äquivalent zu den Standardfehlern des ursprünglichen Modells sind (i.e. "schadet nicht").

Bootstrapped SEs wiederum erfordern mehr Aufwand, ähneln aber den robusten SEs sowieso sehr stark, weshalb sich der Aufwand ggf. nicht wirklich lohnt.

Siehe @hilbeModelingCountData2014 (p. 99f) für eine ausführlichere Beshreibung.

### Underdispersion

Underdispersion entsteht, wenn die Daten eine geringere Varianz aufweisen, als auf Basis eines Poisson- oder NB-Modells erwartet würde.
Bei nichtberücksichtigter underdispersion werden die Standardfehler des Modells überschätzt [@hilbeModelingCountData2014, p. 210].

Im Kontext von hurdle models taucht folgende Bemerkung auf:

> [...] that underdispersion occurs if zeros are less frequent than the parent distribution would predict.
> The higher the expected value of the Poisson distribution, the lowert the predicted probability of zero outcome and the lower the scope for underdispersion.
> -- [@winkelmannEconometricAnalysisCount2010, p. 180]

Im Allgemeinen wird [generalized Poisson](https://stats.stackexchange.com/a/237177/80056) (R: [`VGAM`](https://rdrr.io/cran/VGAM/man/genpoisson.html)) empfohlen [@hilbeModelingCountData2014].



### Beispiel nach @hilbeModelingCountData2014, p. 211ff

Grundlage ist der Datensatz `azprocedure` (siehe Abschnitt \@ref(data)) zur Dauer des Krankenhausaufenthalts nach einer von zwei kardiovaskulären Operationen mit folgenden hier relevanten Variablen:

- `los`: Length of hospital stay
- `sex`: Male (1), female(0)
- `admit`: Type of admission. Urgent/emergency (1), elective (0)

Zuerst werfen wir einen Blick auf die Daten und fitten ein reguläres Poissonmodell.

```{r underdisp_azprocedure_over}
data(azprocedure, package = "COUNT")

describe_counts(azprocedure$los)

# Barchart: So grob poissonverteilt?
ggplot(data = azprocedure, aes(x = los)) +
  geom_bar(alpha = .75) +
  labs(
    title = "Hospital length of stay (LOS)",
    subtitle = "azprocedure data (Hilbe 2014)",
    x = "LOS", y = "Count" 
  )

# Model fit
model_azproc <- glm(los ~ procedure + sex + admit, 
                    data = azprocedure, family = poisson())
 
# Model output (ohne exponentierte Koeffizienten)
pander(model_azproc)

# Pearson Dispersion:
dispersion(model_azproc)
```

Der Dispersionsindex lässt auf overdispersion schließen.  
Betrachten wir ein Subset der Daten, indem wir nur Beobachtungen mit $\mathtt{LOS} \le 8$ betrachten, erhalten wir ein anderes Bild:

```{r underdisp_azprocedure_under}
azprocedure_subset <- subset(azprocedure, los <= 8)

describe_counts(azprocedure_subset$los)

# Barchart
ggplot(data = azprocedure_subset, aes(x = los)) +
  geom_bar(alpha = .75) +
  labs(
    title = "Hospital length of stay (LOS)",
    subtitle = "Subset (LOS <= 8) der azprocedure data (Hilbe 2014)",
    x = "LOS", y = "Count" 
  )

model_azproc_u <- glm(los ~ procedure + sex + admit, 
                      data = azprocedure_subset, family = poisson())

pander(model_azproc_u)
dispersion(model_azproc_u)
```

In diesem Fall haben wir es mit underdispersion zu tun.

## Zero-Inflation (ZI) {#zeros}

Problem: "Excess zeros", i.e. das Modell sagt für $P(y_i = 0\ |\ x_i)$ eine deutlich kleinere Wahrscheinlichkeit vor, als in gegebenen Daten tatsächlich vorliegen.

Zero-inflation hängt eng mit overdispersion zusammen. Sie *kann* Ursache für overdispersion sein, allerdings bedeutet das nicht direkt, dass auch ein entsprechend auf *ZI*-fokussiertes Modell (ZIP, ZINB...) verwendet werden *muss* – ggf. lässt sich die overdispersion bereits durch ein anderes geeignetes Modell "auffangen". Zum Beispiel zeigt @winkelmannEconometricAnalysisCount2010, dass die *NB* allgemein eine größere Anzahl an Nullen erwartet als die Poisson.


```{r excess_zeros_viz, echo=FALSE}
tibble(
  x = 0:15,
  pois = dpois(x, 5) * 100,
  y0 = round(ifelse(x == 0, 5, pois)),
  ymod = dpois(x, mean(y0)) * 100
) %>%
  gather(dist, prob, y0, ymod) %>%
  ggplot(aes(x = x, y = prob, fill = dist)) +
  geom_col(position = "dodge", alpha = .75) +
  scale_x_continuous() +
  scale_fill_brewer(palette = "Set1", labels = c("y0" = "Beobachtet", "ymod" = "Fitted")) +
  labs(
    title = "Zero-Inflation: Simuliertes Beispiel",
    subtitle = "Beobachtete und gefittete Werte bei excess zeros",
    x = "Y", y = "Count", fill = ""
  ) +
  annotate("text", label = "Nullen beobachtet", x = -.3, y = 9, 
           angle = 270, hjust = 1) +
  annotate("text", label = "Nullen erwartet", x = .3, y = 9, 
           angle = 270, hjust = 1) +
  annotate("segment", x = -0.25, xend = -0.25, y = 8.5, yend = 5.5, 
           arrow = arrow(angle = 40, length = unit(.2, "cm")),
           lineend = "round", linejoin = "mitre") +
  annotate("segment", x = 0.25, xend = 0.25, y = 8.5, yend = 1, 
           arrow = arrow(angle = 40, length = unit(.2, "cm")),
           lineend = "round", linejoin = "mitre") +
  theme(legend.position = "top")
```


### Tests auf Zero-Inflation {-}

Zwar schlagen @perumean-chaneyZeroinflatedOverdispersedWhat2013 den Vuong Test (siehe @vuongLikelihoodRatioTests1989 und insbesondere @desmarais2013TestingZero zur Anwendung) vor, um reguläre Modelle mit ihren zero-inflated Gegenstücken zu vergleichen (e.g Poisson vs. ZIP oder NB vs. ZINB), allerdings spricht sich @wilson2015MisuseVuong explizit *gegen* den Vuong-Test aus:

> The misuse of the test stems from misunderstanding of what is meant by the terms "non-nested model" and "nested model". As is the case with many frequently used terms their meanings are approximately understood by many, but precisely understood by few.
>
> --- @wilson2015MisuseVuong, p. 52

(Hausman Test ?)

Für sowohl overdispersion als auch zero-inflation: Score-Tests (?) [@perumean-chaneyZeroinflatedOverdispersedWhat2013, p. 1674, siehe auch entsprechende citations]

> As noted by Yang et al., the univariate situation is recommended for asssessing zero inflation as covariates reduce the power to detect zero inflation
>
> --- @perumean-chaneyZeroinflatedOverdispersedWhat2013, p. 1674

### Modellierung {-}

Für die Modellierung von zero-inflated Daten stehen primär zwei Möglichkeiten zur Verfügung (vgl. auch @hilbeModelingCountData2014, p. 19):

1. *Hurdle models*: Getrennte Modellierung von Nullen und positiven counts in zwei separaten Modellen (siehe Abschnitt \@ref(mod-hurdle)).
2. *Zero-inflated models*: Mixture models, die aus zwei Wahrscheinlichkeitsfunktionen eine neue Wahrscheinlichkeitsfunktion generieren, die nun sowohl Nullen als auch positive counts beschreibt (siehe Abschnitt \@ref(mod-zi)).

Ob hurdle model oder zero-inflated model die bessere Wahl ist hängt mitunter davon ab, welche Annahmen über die _Ursache der Nullen_ getroffen werden können.  
Wenn es eine echte Trennung der Mechanismen (*"separation of mechanisms"*) gibt, die die Nullen und die positiven counts verursachen, dann wäre ein hurdle model eher angemessen.  
Wenn sich die Nullen überlappen, es also keine getrennten Prozesse zu geben scheint, dann wären zero-inflated models angemessen [@hilbeModelingCountData2014, p. 209].

Als Veranschaulichung dieser getrennten Mechanismen können wir das *fish*-Beispiel auf [dieser UCLA-Tutorialseite verwenden](https://stats.idre.ucla.edu/sas/output/zero-inflated-poisson-regression/). Hier ist die Anzahl der gefangenen Fische an einem Wochenende in einem Park die Zählvariable.

```{r fishing_zeros}
# Mittelwert & Varianz von 'count'
describe_counts(fish$count)

# Barchart für counts <= 50, Spannweite sehr groß
fish %>%
  filter(count <= 50) %>%
  ggplot(aes(x = count)) +
  geom_bar(alpha = .75) +
  scale_x_continuous() +
  labs(
    title = "'fish': Anzahl gefangener Fische",
    subtitle = "Limitiert auf counts unter 50",
    x = "Anzahl gefangener Fischer", y = "Count"
  )
```

Die Anzahl der Nullen scheint hier auffallend groß zu sein, allerdings beobachten wir hier auch zwei unterschiedliche Mechanismen: Eine Gruppe kann das ganze Wochenende geangelt haben, während eine andere Gruppe gar nicht geangelt hat – beide Gruppen werden jedoch am Ende nach der Anzahl ihrer gefangenen Fische befragt.  

